import asyncio
import json
from typing import Dict, List
from llama_index.core.tools import FunctionTool
from llama_index.core.llms import ChatMessage
import re
import xml.etree.ElementTree as ET
from pydantic import BaseModel, Field
from typing import Any, Optional, Dict

from llama_index.core.llms import ChatMessage, LLM
from llama_index.core.workflow import (
    Event, StartEvent, StopEvent, step,
    Workflow
)
from llama_index.core.agent.workflow import AgentWorkflow
from pyexpat.errors import messages

from tools.calculator import *
from tools.rag_tools import *
from llama_index.core.llms import ChatMessage
from llama_index.core.agent.workflow import FunctionAgent
from tool_call_llm import DoubaoToolLLM
from llama_index.utils.workflow import draw_all_possible_flows



# --- 1. å®šä¹‰å·¥ä½œæµä¸­çš„æ•°æ®ç»“æ„ (Events and Plan) ---

class UserInputEvent(StartEvent):
    user_msg: str
    chat_history: List[Dict[str, Any]] = Field(default_factory=list)

class FinalOutputEvent(StopEvent):
    response: str

class CalculationEvent(Event):
    user_msg: str
    calculation_details: str

class DocumentSearchEvent(Event):
    user_msg: str
    query: str

class GraphQueryEvent(Event):
    user_msg: str
    query: str

class SimpleChatEvent(Event):
    user_msg: str

# --- æ±‡æ€»äº‹ä»¶ ---
class SummaryRequestEvent(Event):
    user_msg: str
    tool_result: str

# --- 2. å®šä¹‰è§„åˆ’è€…çš„ Prompt ---

DISPATCHER_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åˆ†å‘æœºå™¨äººã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·çš„è¯·æ±‚ï¼Œå¹¶ç»“åˆâ€œèŠå¤©å†å²â€çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶åˆ¤æ–­å®ƒå±äºå“ªä¸€ç±»ä»»åŠ¡ã€‚

**èŠå¤©å†å²**
{chat_history}

## ä»»åŠ¡ç±»åˆ«åŠè¯¦ç»†è¯´æ˜ ##

1.  **"calculation" (è®¡ç®—ä»»åŠ¡)**
    *   **ä½•æ—¶ä½¿ç”¨:** å½“ç”¨æˆ·çš„è¯·æ±‚æ˜¯ä¸€ä¸ªæ˜ç¡®çš„ã€å¯ä»¥ä»£å…¥å…¬å¼è¿›è¡Œè®¡ç®—çš„æ•°å­¦é—®é¢˜æ—¶ã€‚
    *   **å…³é”®è¯:** â€œè®¡ç®—â€ã€â€œæ”¶ç›Šç‡â€ã€â€œæ³¢åŠ¨ç‡æ˜¯å¤šå°‘â€ã€â€œç™¾åˆ†æ¯”â€ã€â€œå¢é•¿äº†å¤šå°‘â€ã€‚
    *   **ç¤ºä¾‹:** "è‚¡ä»·ä»100æ¶¨åˆ°105ï¼Œæ”¶ç›Šç‡æ˜¯å¤šå°‘ï¼Ÿ" 
    *   **åä¾‹:** "Aå…¬å¸çš„æ”¶å…¥å¢é•¿ç‡æ˜¯å¤šå°‘ï¼Ÿ" -> è¿™ä¸æ˜¯è®¡ç®—ä»»åŠ¡ï¼Œå› ä¸ºéœ€è¦å…ˆæŸ¥æ‰¾æ•°æ®ï¼Œæ‰€ä»¥åº”å½’ç±»ä¸º "document_search"ã€‚

2.  **"document_search" (æ–‡æ¡£æ£€ç´¢ä»»åŠ¡)**
    *   **ä½•æ—¶ä½¿ç”¨:** å½“ç”¨æˆ·çš„é—®é¢˜éœ€è¦é€šè¿‡**é˜…è¯»å’Œç†è§£**æ–‡æ¡£ã€æŠ¥å‘Šã€æ–°é—»ç­‰éç»“æ„åŒ–æ–‡æœ¬æ¥å¯»æ‰¾**äº‹å®ã€æ‘˜è¦ã€åŸå› ã€å½±å“ã€è§‚ç‚¹æˆ–è§£é‡Š**æ—¶ã€‚è¿™æ˜¯æœ€å¸¸ç”¨çš„ç±»åˆ«ã€‚
    *   **å…³é”®è¯:** â€œæ˜¯ä»€ä¹ˆâ€ã€â€œä¸ºä»€ä¹ˆâ€ã€â€œåˆ†æâ€ã€â€œæ€»ç»“â€ã€â€œä»‹ç»ä¸€ä¸‹â€ã€â€œæœ‰å“ªäº›â€ã€â€œçš„åŸå› â€ã€â€œçš„å½±å“â€ã€‚
    *   **ç¤ºä¾‹:** "åˆ†æä¸€ä¸‹ç¾å€ºå¯¹ä¸­å›½è¿›å‡ºå£çš„å½±å“" 

3.  **"graph_query" (å›¾è°±æŸ¥è¯¢ä»»åŠ¡)**
    *   **ä½•æ—¶ä½¿ç”¨:** å½“ç”¨æˆ·çš„é—®é¢˜æ˜¯å…³äº**ä¸¤ä¸ªæˆ–å¤šä¸ªç‰¹å®šå®ä½“ä¹‹é—´**çš„ã€**éå¸¸æ˜ç¡®çš„ã€å·²çŸ¥çš„ç»“æ„åŒ–å…³ç³»**æ—¶,å…·æœ‰ä¸»è°“å®¾çš„ç»“æ„ã€‚
    *   **å…³é”®è¯:** â€œå’Œ...çš„å…³ç³»â€ã€â€œåŠå¯¼ä½“å¸‚åœºé”€é‡æ˜¯...â€ã€â€œå“ªäº›å…¬å¸æŠ•èµ„äº†...â€ã€â€œ...çš„è‚¡ä¸œæ˜¯è°â€ã€‚
    *   **ç¤ºä¾‹:** "èƒœåˆ©è¯åˆ¸å’ŒOSLæ•°å­—è¯åˆ¸æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ"

4.  **"simple_chat" (ç®€å•èŠå¤©)**
    *   **ä½•æ—¶ä½¿ç”¨:** å½“ç”¨æˆ·çš„è¯·æ±‚æ˜¯ç®€å•çš„é—®å€™ã€é—²èŠï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªä¸å±äºä»¥ä¸Šä»»ä½•ç±»åˆ«çš„å¸¸è¯†æ€§é—®é¢˜ã€‚
    *   **ç¤ºä¾‹:** "ä½ å¥½"

å¯¹äºéè®¡ç®—çš„é—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨"document_search"
è¯·æ ¹æ®ç”¨æˆ·çš„è¯·æ±‚å’ŒèŠå¤©å†å²ï¼Œåªè¿”å›ä¸€ä¸ªåŒ…å« `task_type` å’Œ `query` çš„ã€ä¸¥æ ¼çš„ JSON å¯¹è±¡ã€‚`query` åº”è¯¥æ˜¯ä¼ é€’ç»™ä¸‹ä¸€æ­¥ä¸“å®¶çš„ã€ç²¾ç‚¼åçš„æŒ‡ä»¤ã€‚



ä¾‹å¦‚ï¼š
ç”¨æˆ·è¯·æ±‚: "Aå…¬å¸è‚¡ç¥¨Açš„2025-8-25æ”¶ç›˜ä»·ä¸º56.7ï¼Œåœ¨2025-8-26æ”¶ç›˜ä»·ä¸º58.4ï¼Œè¯·é—®ä»–çš„æ”¶ç›Šç‡æ˜¯å¤šå°‘ï¼Ÿ"
ä½ çš„è¾“å‡º:
```json
{{
  "task_type": "calculation",
  "query": "ä½¿ç”¨èµ·å§‹ä»·æ ¼56.7å’Œç»“æŸä»·æ ¼58.4ï¼Œè°ƒç”¨calculate_return_rateå·¥å…·è®¡ç®—æ”¶ç›Šç‡"
}}

## å½“å‰ç”¨æˆ·è¯·æ±‚ ##
{user_msg}
"""


# --- 3. åˆ›å»ºè‡ªå®šä¹‰å·¥ä½œæµ ---

class FinancialWorkflow(Workflow):
    def __init__(self, llm: LLM, agents: Dict[str, AgentWorkflow], verbose: bool = False):
        super().__init__(
            timeout=300.0,
            verbose=verbose,
        )

        self.llm = llm
        self.agents = agents
        self.verbose = verbose

        self.tools = {
            "Calculator": [
                return_rate_tool,
                volatility_tool
            ],
            "DocumentSearch": [custom_vector_rag_tool],
            "KnowledgeGraph": [custom_graph_rag_tool],
        }
    # æ­¥éª¤ 1: è§„åˆ’ (Plan)
    @step
    async def dispatcher_step(
            self, ev: UserInputEvent
    ) -> CalculationEvent | DocumentSearchEvent | GraphQueryEvent | SimpleChatEvent:
        if self.verbose: print(f"--- [Dispatcher]: æ­£åœ¨åˆ†æè¯·æ±‚ '{ev.user_msg}'... ---")

        formatted_history_parts = []
        for msg in ev.chat_history:
            # è·å– role
            role = msg.get("role", "unknown").capitalize()

            # ä» blocks åˆ—è¡¨ä¸­æå–æ–‡æœ¬å†…å®¹
            content_text = ""
            blocks = msg.get("blocks", [])
            if blocks and isinstance(blocks, list):

                text_blocks = [b.get("text") for b in blocks if b.get("block_type") == "text" and b.get("text")]
                content_text = "\n".join(text_blocks)

            if content_text:
                formatted_history_parts.append(f"{role}: {content_text}")

        formatted_history = "\n".join(formatted_history_parts)
        if not formatted_history:
            formatted_history = "æ— å†å²è®°å½•ã€‚"

        prompt = DISPATCHER_PROMPT.format( chat_history=formatted_history, user_msg=ev.user_msg)

        response = await self.llm.achat(
            messages=[ChatMessage(role="user", content=prompt)],
        )

        try:

            # 1. é¦–å…ˆï¼Œè·å–åŸå§‹ content
            raw_content = response.message.content
            if self.verbose: print(
                f"---  [Dispatcher Step 1]: LLM è¿”å›çš„åŸå§‹ content ç±»å‹: {type(raw_content)}, å†…å®¹: '{raw_content}' ---")

            # 2. æ£€æŸ¥ content æ˜¯å¦ä¸º None æˆ–ç©º
            if not raw_content or not raw_content.strip():
                raise ValueError("LLM returned empty content.")

            # 3. æ‰§è¡Œæ¸…ç†æ“ä½œ
            cleaned_response = raw_content.strip().replace("```json", "").replace("```", "")

            final_json_str = cleaned_response.strip()
            if self.verbose: print(f"---  [Dispatcher Step 3]: æœ€ç»ˆå‡†å¤‡è§£æçš„å­—ç¬¦ä¸²: '{final_json_str}' ---")

            decision = json.loads(final_json_str)
            task_type = decision.get("task_type")
            query = decision.get("query")

            if self.verbose: print(f"--- [Dispatcher]: å†³ç­–å®Œæˆï¼Œä»»åŠ¡ç±»å‹ = {task_type} ---")

            if task_type == "calculation":
                return CalculationEvent(user_msg=ev.user_msg, calculation_details=query)
            elif task_type == "document_search":
                return DocumentSearchEvent(user_msg=ev.user_msg, query=query)
            elif task_type == "graph_query":
                return GraphQueryEvent(user_msg=ev.user_msg, query=query)
            else:
                return SimpleChatEvent(user_msg=ev.user_msg)
        except Exception as e:
            error_context = "cleaned_response æœªå®šä¹‰"
            if 'cleaned_response' in locals():
                error_context = f"cleaned_response çš„å€¼ä¸º: '{cleaned_response}'"

            if self.verbose: print(f"--- âŒ [Dispatcher]: å†³ç­–è§£æå¤±è´¥ ({e})ï¼Œä¸Šä¸‹æ–‡: {error_context}ï¼Œè½¬ä¸ºç®€å•èŠå¤©ã€‚ ---")
            return SimpleChatEvent(user_msg=ev.user_msg)

    async def _execute_tool_calling_step(self, tool_category: str, user_input: str) -> str:
        """ä¸€ä¸ªé€šç”¨çš„ã€æ‰§è¡Œ Tool Calling çš„è¾…åŠ©å‡½æ•°"""
        target_tools = self.tools[tool_category]
        if self.verbose: print(f"--- [{tool_category}]: æ¥æ”¶åˆ°ä»»åŠ¡ '{user_input}' ---")

        # é˜¶æ®µ 1: æ¨¡å‹å†³ç­–
        response = await self.llm.achat(
            messages=[ChatMessage(role="user", content=user_input)],
            tools=target_tools,
            tool_choice="auto",
        )

        tool_calls = response.message.additional_kwargs.get("tool_calls")

        if not tool_calls:
            if self.verbose: print(f"--- [{tool_category}]: LLM å†³å®šä¸è°ƒç”¨å·¥å…·ï¼Œç›´æ¥å›ç­”ã€‚---")
            return response.message.content

        # é˜¶æ®µ 2: æœ¬åœ°æ‰§è¡Œ
        if self.verbose: print(f"--- ğŸ”§ [{tool_category}]: LLM å†³å®šè°ƒç”¨å·¥å…·... ---")
        tool_outputs = []
        for call in tool_calls:
            tool_name = call.get("function", {}).get("name")
            arguments_str = call.get("function", {}).get("arguments")

            if not tool_name or arguments_str is None:
                tool_outputs.append({"output": f"é”™è¯¯: LLM è¿”å›çš„ tool_call æ ¼å¼ä¸å®Œæ•´: {call}"})
                continue
            arguments = json.loads(arguments_str)

            target_tool = next((t for t in target_tools if t.metadata.name == tool_name), None)
            if not target_tool:
                tool_outputs.append({"output": f"é”™è¯¯: æ‰¾ä¸åˆ°åä¸º '{tool_name}' çš„å·¥å…·å¯¹è±¡"})
                continue

            target_tool_fn = target_tool.fn

            #   æˆ‘ä»¬ç°åœ¨è¦åˆ¤æ–­ target_tool_fn æ˜¯åŒæ­¥è¿˜æ˜¯å¼‚æ­¥
            if asyncio.iscoroutinefunction(target_tool_fn):
                # å¦‚æœæ˜¯å¼‚æ­¥å‡½æ•° (async def)ï¼Œå°±ç›´æ¥ await
                if self.verbose: print(f"      - (Async) æ‰§è¡Œå‡½æ•°: {tool_name}(**{arguments})")
                tool_result = await target_tool_fn(**arguments)
            else:
                # å¦‚æœæ˜¯åŒæ­¥å‡½æ•° (def)ï¼Œå°±ç”¨ asyncio.to_thread åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ
                if self.verbose: print(f"      - (Sync via Thread) æ‰§è¡Œå‡½æ•°: {tool_name}(**{arguments})")
                tool_result = await asyncio.to_thread(target_tool_fn, **arguments)

            tool_outputs.append({"tool_name": tool_name, "output": str(tool_result)})


        final_result = "\n".join([f"{r['tool_name']} è¿”å›: {r['output']}" for r in tool_outputs])
        return final_result

    # æ­¥éª¤ 2a: è®¡ç®—åˆ†æ”¯

    @step
    async def calculation_step(self, ev: CalculationEvent) -> SummaryRequestEvent:
        result = await self._execute_tool_calling_step("Calculator", ev.calculation_details)
        return SummaryRequestEvent(user_msg=ev.user_msg, tool_result=result)

    # æ­¥éª¤ 2b: æ–‡æ¡£æ£€ç´¢åˆ†æ”¯
    @step
    async def doc_search_step(self, ev: DocumentSearchEvent) -> SummaryRequestEvent:
        result = await self._execute_tool_calling_step("DocumentSearch", ev.query)
        return SummaryRequestEvent(user_msg=ev.user_msg, tool_result=result)

    # æ­¥éª¤ 2c: å›¾è°±æŸ¥è¯¢åˆ†æ”¯
    @step
    async def graph_query_step(self, ev: GraphQueryEvent) -> SummaryRequestEvent:
        result = await self._execute_tool_calling_step("KnowledgeGraph", ev.query)
        return SummaryRequestEvent(user_msg=ev.user_msg, tool_result=result)

    # æ­¥éª¤ 2d: ç®€å•èŠå¤©åˆ†æ”¯
    @step
    async def simple_chat_step(self, ev: SimpleChatEvent) -> FinalOutputEvent:
        if self.verbose: print(f"--- ğŸ’¬ [Chat]: æ­£åœ¨ç›´æ¥å›ç­”... ---")
        messages_to_send = [ChatMessage(role="user", content=ev.user_msg)]
        response = await self.llm.achat(messages_to_send)
        return FinalOutputEvent(response=response.message.content)

    # æ­¥éª¤ 3: æ€»ç»“å™¨ (Summarizer) - æ¥æ”¶æ‰€æœ‰ä¸“å®¶åˆ†æ”¯çš„ç»“æœ
    @step
    async def summarizer_step(self, ev: SummaryRequestEvent) -> FinalOutputEvent:
        if self.verbose: print(f"--- [Summarizer]: æ­£åœ¨æ€»ç»“ä¸“å®¶ç»“æœ... ---")
        prompt = (
            f"ä½ æ˜¯ä¸€ä¸ªæ€»ç»“ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„â€œåŸå§‹è¯·æ±‚â€å’Œä¸‹é¢ä¸“å®¶çš„â€œå·¥ä½œæŠ¥å‘Šâ€ï¼Œç”Ÿæˆä¸€ä¸ªæœ€ç»ˆçš„ã€æµç•…è¿è´¯çš„ä¸­æ–‡å›ç­”ã€‚\n\n"
            f"## ç”¨æˆ·çš„åŸå§‹è¯·æ±‚ ##\n{ev.user_msg}\n\n"
            f"## ä¸“å®¶å·¥ä½œæŠ¥å‘Š ##\n{ev.tool_result}\n\n"
            f"## æœ€ç»ˆå›ç­” ##"
        )
        response = await self.llm.achat([ChatMessage(role="user", content=prompt)])
        return FinalOutputEvent(response=response.message.content)

# draw_all_possible_flows(FinancialWorkflow, filename="multi_step_workflow.html")